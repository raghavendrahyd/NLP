{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Attention from Scratch. Specificaly multi headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Headed Attention:\n",
    "\n",
    "Multi Headed Attention is a way to combine multiple attention heads into one. This is done by concatenating the attention heads and then applying a linear transformation to the result. This is done to allow the model to jointly attend to information at different positions from different representational spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ## Prepare for multi-head attention\n",
    "    This module does a linear transformation and splits the vector into given\n",
    "    number of heads for multi-head attention.\n",
    "    This is used to transform **key**, **query**, and **value** vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        super().__init__()\n",
    "        # Linear layer for linear transform\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        # Number of heads\n",
    "        self.heads = heads\n",
    "        # Number of dimensions in vectors in each head\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`.\n",
    "        # We apply the linear transformation to the last dimension and split that into\n",
    "        # the heads.\n",
    "        head_shape = x.shape[:-1]\n",
    "\n",
    "        # Linear transform\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # Split last dimension into heads\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "\n",
    "        # Output has shape `[seq_len, batch_size, heads, d_k]` or `[batch_size, heads, d_model]`\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathop{Attention}(Q, K, V) = \\underset{seq}{\\mathop{softmax}}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This computes scaled multi-headed attention for given `query`, `key` and `value` vectors.\n",
    "    i.e, it finds keys that matches the query, and gets the values of\n",
    "     those keys.\n",
    "    It uses dot-product of query and key as the indicator of how matching they are.\n",
    "    Softmax is calculated along the axis of of the sequence (or time).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        \"\"\"\n",
    "        * `heads` is the number of heads.\n",
    "        * `d_model` is the number of features in the `query`, `key` and `value` vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of features per head\n",
    "        self.d_k = d_model // heads\n",
    "        # Number of heads\n",
    "        self.heads = heads\n",
    "\n",
    "        # These transform the `query`, `key` and `value` vectors for multi-headed attention.\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n",
    "\n",
    "        # Softmax for attention along the time dimension of `key`\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Scaling factor before the softmax\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "\n",
    "        # We store attentions so that it can be used for logging, or other computations if needed\n",
    "        self.attn = None\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ### Calculate scores between queries and keys\n",
    "        This method can be overridden for other variations like relative attention.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate $Q K^\\top$ or $S_{ijbh} = \\sum_d Q_{ibhd} K_{jbhd}$\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "\n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        \"\"\"\n",
    "        `mask` has shape `[seq_len_q, seq_len_k, batch_size]`, where first dimension is the query dimension.\n",
    "        If the query dimension is equal to $1$ it will be broadcasted.\n",
    "        \"\"\"\n",
    "\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        # Same mask applied to all heads.\n",
    "        mask = mask.unsqueeze(-1)\n",
    "\n",
    "        # resulting mask has shape `[seq_len_q, seq_len_k, batch_size, heads]`\n",
    "        return mask\n",
    "\n",
    "    def forward(self, *,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        `query`, `key` and `value` are the tensors that store\n",
    "        collection of *query*, *key* and *value* vectors.\n",
    "        They have shape `[seq_len, batch_size, d_model]`.\n",
    "        `mask` has shape `[seq_len, seq_len, batch_size]` and\n",
    "        `mask[i, j, b]` indicates whether for batch `b`,\n",
    "        query at position `i` has access to key-value at position `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # `query`, `key` and `value`  have shape `[seq_len, batch_size, d_model]`\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        # Prepare `query`, `key` and `value` for attention computation.\n",
    "        # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # Compute attention scores $Q K^\\top$.\n",
    "        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n",
    "        scores = self.get_scores(query, key)\n",
    "\n",
    "        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        scores *= self.scale\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # $softmax$ attention along the key sequence dimension\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        # Save attentions if debugging\n",
    "        tracker.debug('attn', attn)\n",
    "\n",
    "        # Apply dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Multiply by values\n",
    "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "        # Save attentions for any other calculations \n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        # Concatenate multiple heads\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        # Output layer\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example with data for Simple Self attention (single head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(6, 16) # 6 words in vocab, 16 dimensional embeddings\n",
    "embedded_sentence = embed(sentence_int).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
       "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
       "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
       "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
       "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
       "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
       "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
       "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
       "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
       "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
       "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.rand(d_q, d)\n",
    "W_key = torch.rand(d_k, d)\n",
    "W_value = torch.rand(d_v, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 16]), torch.Size([24, 16]), torch.Size([28, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query.shape, W_key.shape, W_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1] # second word\n",
    "x_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8982,  0.1030,  0.4428,  0.6328, -1.7003,  1.3489, -0.3082, -0.5900,\n",
       "        -0.9257, -0.7688,  1.8828, -1.6065, -0.8011, -0.4114, -0.6116,  1.3902,\n",
       "        -0.1460,  0.0244, -0.5577,  1.5972, -2.2190, -0.0214,  0.2002,  1.3752])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query@x_2 # Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8982,  0.1030,  0.4428,  0.6328, -1.7003,  1.3489, -0.3082, -0.5900,\n",
       "        -0.9257, -0.7688,  1.8828, -1.6065, -0.8011, -0.4114, -0.6116,  1.3902,\n",
       "        -0.1460,  0.0244, -0.5577,  1.5972, -2.2190, -0.0214,  0.2002,  1.3752])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24]), torch.Size([24]), torch.Size([28]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2.shape, key_2.shape, value_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 16]), torch.Size([6, 16]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_key.shape, embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = (W_key@ embedded_sentence.T).T\n",
    "values = (W_value@ embedded_sentence.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 24]), torch.Size([6, 28]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1466)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2.dot(keys[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T)\n",
    "omega_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_2 # unnormalized attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458])\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0) # normalized attention weights w.r.t input token 2\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
      "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
      "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
      "        -0.9564, -0.5265,  0.0624,  1.7084])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values) # context vector w.r.t input token 2\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi head attention\n",
    "\n",
    "h = 3\n",
    "\n",
    "multihead_W_query = torch.rand(h, d_q, d)\n",
    "multihead_W_key = torch.rand(h, d_k, d)\n",
    "multihead_W_value = torch.rand(h, d_v, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 24, 16]), torch.Size([3, 24, 16]), torch.Size([3, 28, 16]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_W_query.shape, multihead_W_key.shape, multihead_W_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 24]), torch.Size([3, 28]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)\n",
    "multihead_key_2.shape, multihead_value_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 6])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
       "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
       "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
       "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
       "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
       "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
       "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
       "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
       "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
       "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
       "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374,  0.5146,  0.2553, -1.3250, -0.0770,  0.8768],\n",
       "        [-0.1778,  0.9938, -0.5496,  0.1784, -1.0205,  1.6221],\n",
       "        [-0.3035, -0.2587,  1.0042, -2.1338, -0.1690, -1.4779],\n",
       "        [-0.5880, -1.0826,  0.8272,  1.0524,  0.9178,  1.1331],\n",
       "        [ 0.3486, -0.0444, -0.3948, -0.3885,  1.5810, -1.2203],\n",
       "        [ 0.6603,  1.6236,  0.4892, -0.9343,  1.3010,  1.3139],\n",
       "        [-0.2196, -2.3229, -0.2168, -0.4991,  1.2753,  1.0533],\n",
       "        [-0.3792,  1.0878, -1.7472, -1.0867, -0.2010,  0.1388],\n",
       "        [ 0.7671,  0.6716, -1.6025,  0.8805,  0.4965,  2.2473],\n",
       "        [-1.1925,  0.6933, -1.0764,  1.5542, -1.5723, -0.8036],\n",
       "        [ 0.6984, -0.9487,  0.9031,  0.6266,  0.9666, -0.2808],\n",
       "        [-1.4097, -0.0765, -0.7218, -0.1755, -1.1481,  0.7697],\n",
       "        [ 0.1794, -0.1526, -0.5951,  0.0983, -1.1589, -0.6596],\n",
       "        [ 1.8951,  0.1167, -0.7112, -0.0935,  0.3255, -0.7979],\n",
       "        [ 0.4954,  0.4403,  0.6230,  0.2662, -0.6315,  0.1838],\n",
       "        [ 0.2692, -1.4465, -1.3729, -0.5850, -2.8400,  0.2293]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_inputs[0].permute(1, 0)==stacked_inputs[0].T #checking permute functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 24, 6])\n",
      "multihead_values.shape: torch.Size([3, 28, 6])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 6, 24])\n",
      "multihead_values.shape: torch.Size([3, 6, 28])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('torch_2_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fb8b680fd183c223b836b79cdcb0f2f0334fe2549f7dc1091ef51ac1f51f680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
