{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief about Transformers in NLP\n",
    "\n",
    "Transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is a powerful model that has been applied to a wide range of tasks, including machine translation, text summarization, and question answering. In this notebook, we will learn about the Transformer model and its applications in NLP.\n",
    "\n",
    "### What is a Transformer?\n",
    "\n",
    "The Transformer model was introduced in the paper Attention is All You Need by Vaswani et al. in 2017. It is a deep learning model that uses self-attention to process sequences. It is better than the Recurrent Neural Network (RNN) model in terms of processing long sequences. Primary advantages of the Transformer model are:\n",
    "\n",
    "It is a parallelizable model, which means that it can be trained on multiple GPUs.\n",
    "It is a self-attention model, which means that it can learn the relationships between different parts of the input data.\n",
    "It is a sequence-to-sequence model, which means that it can process sequences of any length.\n",
    "\n",
    "The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "### What is self-attention?\n",
    "\n",
    "Self-attention is a mechanism that allows a model to focus on different parts of the input data. It is a powerful mechanism that allows the model to learn the relationships between different parts of the input data. The self-attention mechanism is used in the Transformer model to process sequences. Self attention is done by calculating a score for each pair of input elements, and then using those scores to weight the contributions of each element to the final output.\n",
    "\n",
    "Self-attention is a powerful mechanism that has revolutionized the field of NLP. It is able to capture long-range dependencies in an input sequence, which is essential for many NLP tasks. Self-attention is also able to be parallelized, which makes it faster to train.\n",
    "\n",
    "A mathematical explanation of self-attention:\n",
    "\n",
    "The self-attention mechanism calculates a score for each pair of input elements, and then uses those scores to weight the contributions of each element to the final output. The self-attention mechanism is as follows:\n",
    "\n",
    "$$\\begin{aligned} \\text{Attention}(Q, K, V) &= \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\\\ \\text{where} \\quad Q, K, V &\\in \\mathbb{R}^{n \\times d_k} \\end{aligned}$$\n",
    "\n",
    "where Q, K, V are the query, key, and value matrices, respectively. The query matrix is used to calculate the scores for each pair of input elements. The key matrix is used to calculate the scores for each pair of input elements. The value matrix is used to calculate the scores for each pair of input elements. The scores are calculated by taking the dot product of the query and key matrices, and then dividing by the square root of the dimension of the key matrix. The scores are then normalized using the softmax function. The final output is calculated by multiplying the scores by the value matrix.\n",
    "\n",
    "Final output of the self-attention process is a weighted sum of the vectors, where the weights are given by:\n",
    "\n",
    "$$y_i = \\sum_{j=1}^n \\alpha_{ij}x_j$$\n",
    "\n",
    "where $\\alpha_{ij}$ is the weight given to $x_j$ by $x_i$. $x_i$ is the query vector, and $x_j$ is the key vector. The weights are calculated by:\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp(\\frac{q_i^Tk_j}{\\sqrt{d_k}})}{\\sum_{l=1}^n \\exp(\\frac{q_i^Tk_l}{\\sqrt{d_k}})}$$\n",
    "\n",
    "where $q_i$ is the query vector, and $k_j$ is the key vector. The weights are calculated by taking the dot product of the query and key vectors, and then dividing by the square root of the dimension of the key vector. The weights are then normalized using the softmax function.\n",
    "\n",
    "###  Transformer architecture\n",
    "\n",
    "The Transformer model is a sequence-to-sequence model that uses self-attention to process sequences. The Transformer model is composed of encoders and a decoders.\n",
    "\n",
    "The encoders are all identical in structure (yet they do not share weights). The decoder is also identical in structure to the encoders (yet they do not share weights). The Transformer model is as follows:\n",
    "\n",
    "$$\\begin{aligned} \\text{Encoder} &= \\text{EncoderLayer} \\circ \\dots \\circ \\text{EncoderLayer} \\\\ \\text{Decoder} &= \\text{DecoderLayer} \\circ \\dots \\circ \\text{DecoderLayer} \\\\ \\text{Transformer} &= \\text{Encoder} \\circ \\text{Decoder} \\end{aligned}$$\n",
    "\n",
    "where $\\circ$ is the concatenation operator. \n",
    "\n",
    "The encoder is composed of multiple layers. Each layer is composed of a self-attention mechanism, a feed-forward neural network, and a residual connection. The self-attention mechanism is used to process the input sequence. The feed-forward neural network is used to process the output of the self-attention mechanism. The residual connection is used to pass the input to the output of the feed-forward neural network.\n",
    "\n",
    "The decoder is composed of multiple layers. Every decoder has two self-attention mechanisms, a feed-forward neural network, and a residual connection. The first self-attention mechanism is used to process the input sequence. The second self-attention mechanism is used to process the output of the first self-attention mechanism. The feed-forward neural network is used to process the output of the second self-attention mechanism. The residual connection is used to pass the input to the output of the feed-forward neural network.\n",
    "\n",
    "The parallelization is acheived through the use of multiple encoders and decoders as well as by the way we feed the data into the model. Inside the multi head self attention, all the words of the sentence are compared to all the words of the sentence. This is done in parallel. Whereas in feed forward neural network, the words are processed separately not allowing for exchange of information between the words.\n",
    "\n",
    "Note all the inputs are embedded. Positional embeddings are added during input.\n",
    "Lets talk positional embeddings in more detail:\n",
    "\n",
    "#### Positional embeddings\n",
    "\n",
    "As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model. This is where positional embeddings come in. The Transformer model uses positional embeddings to encode the position of each word in the input sequence. The positional embeddings are added to the word embeddings to create the input embeddings. The positional embeddings are as follows:\n",
    "\n",
    "$$\\begin{aligned} \\text{PE}(pos, 2i) &= \\sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\ \\text{PE}(pos, 2i+1) &= \\cos(pos / 10000^{2i/d_{\\text{model}}}) \\end{aligned}$$\n",
    "\n",
    "where $pos$ is the position of the word in the input sequence, and $d$ is the dimension of the embedding and $i$ is the index of the embedding. The positional embeddings are added to the word embeddings to create the input embeddings.\n",
    "\n",
    "i.e \n",
    "positional embedding[pos, i] = \n",
    "\n",
    "            sin(pos / 10000^(2i/dmodel))    if i is even,\n",
    "\n",
    "            cos(pos / 10000^(2i/d_model))    if i is odd\n",
    "\n",
    "For example, if the input sequence is \"this is the sentence\" and d_model = 4, then the positional embeddings are as follows:\n",
    "\n",
    "The first token \"This\" has position index 0. Its positional embedding is:\n",
    "\n",
    "| pos | index | formula = pos_embedding[pos, index]  |\n",
    "|-----|-------|--------------------------------------|\n",
    "| 0   | 0     | sin(0 / 10000^(2*0/4)) = 0             |\n",
    "| 0   | 1     | cos(0 / 10000^(2*1/4)) = 1             |\n",
    "| 0   | 2     | sin(0 / 10000^(2*2/4)) = 0             |\n",
    "| 0   | 3     | cos(0 / 10000^(2*3/4)) = 1             |\n",
    "\n",
    "i.e the positional embedding for the first token is [0, 1, 0, 1].\n",
    "\n",
    "The second token \"is\" has position index 1. Its positional embedding is:\n",
    "\n",
    "| pos | index | formula = pos_embedding[pos, index]  |\n",
    "|-----|-------|--------------------------------------|\n",
    "| 1   | 0     | sin(1 / 10000^(2*0/4)) = 0.841471     |\n",
    "| 1   | 1     | cos(1 / 10000^(2*1/4)) = 0.99995     |\n",
    "| 1   | 2     | sin(1 / 10000^(2*2/4)) = 9.999999983333334e-05     |\n",
    "| 1   | 3     | cos(1 / 10000^(2*3/4)) = 0.9999    |\n",
    "\n",
    "i.e the positional embedding for the second token is [0.841471, 0.99995, 9.999999983333334e-05, 0.9999].\n",
    "\n",
    "They could used some other methodology for positional however the following criteria should be satisfied for the positional embeddings, which is conveniently satisfied by the above method:\n",
    "\n",
    "- It should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "- Distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "- Should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "- It must be deterministic.\n",
    "\n",
    "Positional embeddings could be learned during too. However, the above method is used as it is simple and works well.\n",
    "\n",
    "The sine and cosine functions are used to encode the position of the word in the sequence because they are periodic functions. This means that they repeat themselves at regular intervals. This is important because the position of a word in a sequence is a relative concept. The position of a word only has meaning in the context of the other words in the sequence. Also, the use of the sine and cosine functions allows the positional embeddings to be independent of the order of the words in the sequence. This is important because the order of the words in a sequence is not always preserved in NLP tasks. For example, in a question answering task, the order of the words in the question is not preserved in the answer. The positional embeddings are also scaled by a factor of $10000^{2i/d_{\\text{model}}}$ to ensure that the positional embeddings have a similar scale to the word embeddings. The positional embeddings are then added to the word embeddings, which are then used as input to the self-attention layers\n",
    "\n",
    "\n",
    "Transformers also use layer normalization. Layer normalization is a technique for normalizing the activations of a neural network layer. It is a type of batch normalization. Layer normalization is used to normalize the activations of the layer for each given example in a batch instead of normalizing the activations of the layer across a batch like batch normalization.\n",
    "\n",
    "References/ Further reading:\n",
    "\n",
    "https://arxiv.org/abs/1706.03762 - Attention is all you need\n",
    "\n",
    "https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "\n",
    "https://www.youtube.com/watch?v=_UVfwBqcnbM\n",
    "\n",
    "https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
    "\n",
    "http://nlp.seas.harvard.edu/annotated-transformer/\n",
    "\n",
    "https://youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from scratch\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        depth here implies the number of features in the input\n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = k.shape[-1]\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights # output is the attention vector and attention_weights is the attention matrix\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model # embedding dimension. Implies each word is represented by a vector of size d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0 # d_model must be divisible by num_heads\n",
    "\n",
    "        self.depth = d_model // self.num_heads # depth or head dimension is the number of features in each head\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model) # linear layer for query\n",
    "        self.wk = nn.Linear(d_model, d_model) # linear layer for key\n",
    "        self.wv = nn.Linear(d_model, d_model) # linear layer for value\n",
    "\n",
    "        self.dense = nn.Linear(d_model, d_model) # linear layer for output\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, v, k, q, mask = None):\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(d_model, dff), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(dff, d_model)\n",
    "    )\n",
    "\n",
    "def positional_encoding(maximum_position_encoding, d_model):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        maximum_position_encoding: the maximum position to be encoded\n",
    "        d_model: the embedding dimension\n",
    "    Returns:\n",
    "        pos_encoding: a tensor of shape (1, maximum_position_encoding, d_model) containing the positional encoding\n",
    "    \"\"\"\n",
    "    pos_encoding = np.array([[pos / np.power(10000, 2 * (i // 2) / d_model) for i in range(d_model)] for pos in range(maximum_position_encoding)])\n",
    "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2]) # dim 2i, even\n",
    "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2]) # dim 2i+1, odd\n",
    "    pos_encoding = torch.tensor(pos_encoding, dtype = torch.float32) # convert to tensor\n",
    "    pos_encoding = pos_encoding.unsqueeze(0) # add a batch dimension\n",
    "    return pos_encoding # (1, maximum_position_encoding, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            num_layers: number of encoder layers\n",
    "            d_model: embedding dimension\n",
    "            num_heads: number of attention heads\n",
    "            dff: dimension of feed forward network\n",
    "            input_vocab_size: size of the input vocabulary\n",
    "            maximum_position_encoding: maximum position encoding\n",
    "            rate: dropout rate\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model # embedding dimension\n",
    "        self.num_layers = num_layers # number of encoder layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model) # embedding layer for input\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model) # positional encoding for input sequence\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, training, mask):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            x: input sequence. shape = (batch_size, input_seq_len)\n",
    "            training: boolean for training or inference\n",
    "            mask: padding mask\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)) # scaling by sqrt(d_model). Scaling helps the model to learn better\n",
    "        x += self.pos_encoding[:, :seq_len, :] # positional encoding is added to the embedding\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 1st masked multi-head attention for decoder. This is used to look at the previous words in the target sequence\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # 2nd multi-head attention for decoder. This is used to look at the encoder output and the previous words in the target sequence \n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model) # layer normalization for 1st masked multi-head attention\n",
    "        self.layernorm2 = nn.LayerNorm(d_model) # layer normalization for 2nd multi-head attention\n",
    "        self.layernorm3 = nn.LayerNorm(d_model) # layer normalization for feed forward network\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, enc_output, training, \n",
    "                look_ahead_mask, padding_mask):\n",
    "\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # 1st masked multi-head attention (self attention)\n",
    "        # attn1.shape == (batch_size, target_seq_len, d_model)\n",
    "        # attn_weights_block1.shape == (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x) \n",
    "\n",
    "        # 2nd multi-head attention (cross attention). \n",
    "        # Here query is the output of 1st masked multi-head attention and key and value are the encoder output\n",
    "        # attn2.shape == (batch_size, target_seq_len, d_model)\n",
    "        # attn_weights_block2.shape == (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, enc_output, training, \n",
    "                look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = x.shape[1] # target sequence length\n",
    "        attention_weights = {} # dictionary to store attention weights\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)) # scaling by sqrt(d_model). Scaling helps the model to learn better\n",
    "        x += self.pos_encoding[:, :seq_len, :] # positional encoding is added to the embedding\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1 = self.dec_layers[i](x, enc_output, training,\n",
    "                                           look_ahead_mask, padding_mask)\n",
    "                                           # look_ahead_mask is used to mask the future tokens in the target sequence\n",
    "                                            # padding_mask is used to mask the padding tokens in the input sequence\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1 # storing attention weights for 1st masked multi-head attention\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The Transformer class\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "          super(Transformer, self).__init__()\n",
    "    \n",
    "          self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                              input_vocab_size, pe_input, rate)\n",
    "    \n",
    "          self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                              target_vocab_size, pe_target, rate)\n",
    "    \n",
    "          self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, inp, tar, training, enc_padding_mask,\n",
    "                look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('torch_2_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fb8b680fd183c223b836b79cdcb0f2f0334fe2549f7dc1091ef51ac1f51f680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
